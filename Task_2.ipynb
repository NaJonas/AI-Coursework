{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta output 1.5106110122690795\n",
      "delta input [[1.02307009 0.35946786 1.14818439 0.35614362 1.31145313]\n",
      " [0.54436708 0.55136317 0.11142597 0.58024604 0.58790094]\n",
      " [0.26334682 0.79188457 0.33616118 0.18953968 1.35900631]\n",
      " [1.50551877 0.08229742 0.14958062 0.07948892 0.69799874]\n",
      " [0.54143038 0.42470667 0.3060362  1.31617876 0.11467703]\n",
      " [0.43385464 0.45582013 0.52720594 0.49734856 0.76017713]\n",
      " [0.74376981 0.98900474 0.68896893 1.37971687 1.11821505]\n",
      " [1.29386188 0.22761481 0.70497497 0.48000246 0.3902462 ]\n",
      " [0.33391745 0.32593675 1.4150393  0.53844458 1.0689458 ]\n",
      " [0.91582742 1.43010305 1.29512151 1.35810679 1.32124057]]\n",
      "tmp [0.89915899 0.52624919 0.63470596 0.57757636 0.58545711 0.57411715\n",
      " 1.06229931 0.68970902 0.76046139 1.37031425]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,784) (784,10) (5,784) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-0b7ba695ef0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__innit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnupyzdintas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;31m#test.training()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m#test.test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-272-0b7ba695ef0f>\u001b[0m in \u001b[0;36mnupyzdintas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tmp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_input\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_input\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,784) (784,10) (5,784) "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Obtaining and normalizing MNIST dataset helped by: https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
    "# Mean and Standart Deviation 0.5 each for data normalization [-1, 1] \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "\n",
    "                               ])\n",
    "# No normalization\n",
    "transform2 = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST('', download=True, train=True, transform=transform2)\n",
    "valset = datasets.MNIST('', download=True, train=False, transform=transform2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels_ = dataiter.next()\n",
    "\n",
    "testiter = iter(valloader)\n",
    "images1, labels1 = testiter.next()\n",
    "\n",
    "# Turning the dataset into hardcoded for now 64 size batch of flattened 28*28 pixels\n",
    "# And one hot encoding for the labels\n",
    "\n",
    "x_train = images.numpy()\n",
    "x_train = np.reshape(x_train, (64,28*28))\n",
    "labels = labels_.numpy()\n",
    "labels = labels.reshape(-1, 1)\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit(labels)\n",
    "y_train = ohe.transform(labels).toarray()\n",
    "\n",
    "\n",
    "\n",
    "x_test = images1.numpy()\n",
    "x_test = np.reshape(x_test, (64,28*28))\n",
    "labels1 = labels1.numpy()\n",
    "labels1 = labels1.reshape(-1, 1)\n",
    "ohe.fit(labels1)\n",
    "y_test = ohe.transform(labels1).toarray()\n",
    "\n",
    "\n",
    "\n",
    "#plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "losses=[]\n",
    "\n",
    "# 1 hidden layer, 5 units sigmoid\n",
    "# Weights initialized in random uniform\n",
    "class NeuralNetwork:\n",
    "    def __innit__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.units = 5\n",
    "        self.layers = 2\n",
    "        self.weights_input = np.random.uniform(-1, 1, (self.units, 784))\n",
    "        self.weights_h1 = np.random.uniform(0, 1, (self.units, self.units))\n",
    "        self.weights_output = np.random.uniform(0, 1, (10, self.units))\n",
    "        self.bias_input = np.random.uniform(0,1, (self.units))\n",
    "        self.bias_h1 = np.random.uniform(0,1, (self.units))\n",
    "        self.bias_output = np.random.uniform(0,1, (10))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1-self.sigmoid(x))\n",
    "    \n",
    "    def training(self):\n",
    "        ########################################\n",
    "        for i in range(64):\n",
    "        \n",
    "            sigmoid_activation = self.forward_sigmoid(x_train[i])\n",
    "            softmax_activation = self.softmax_layer(sigmoid_activation)\n",
    "            # Mean squared error function\n",
    "            error = softmax_activation-y_train[i]\n",
    "            #print(\"first weights\",self.weights_input)\n",
    "            # Dot product of the softmax derivate matrix and cost vector\n",
    "            # https://www.python-course.eu/softmax.php\n",
    "            delta_output =  np.dot(error, self.softmax_derivative(softmax_activation))\n",
    "            \n",
    "            #delta_output = np.outer(sigmoid_activation, np.dot(error,self.softmax_derivative(softmax_activation)))\n",
    "            self.bias_output += error\n",
    "            \n",
    "            delta_input = np.dot(delta_output, self.weights_output)\n",
    "            temp = np.dot(delta_input, self.sigmoid_derivative(sigmoid_activation))\n",
    "            \n",
    "            \n",
    "            self.weights_output += np.outer(sigmoid_activation, delta_output).T * self.learning_rate\n",
    "\n",
    "            self.weights_input += np.dot(x_train[0], temp) * self.learning_rate\n",
    "            self.bias_input += delta_input\n",
    "        \n",
    "            # Hidden layer\n",
    "            #hidden_layer_error = np.dot(self.weights_output.T, delta_output)\n",
    "            #hidden_layer_delta = hidden_layer_error * self.sigmoid_derivative(sigmoid_activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "            #weights_input = self.sigmoid_derivative(sigmoid_activation) * hidden_layer_error * x_train[0] \n",
    "        \n",
    "            #self.weights_input += self.learning_rate * (self.weights_input.T @ hidden_layer_delta) \n",
    "            #self.bias_input += hidden_layer_delta\n",
    "            print(\"error\",error)\n",
    "        \n",
    "            #self.weights_output += self.learning_rate  * (delta_output @ softmax_activation)\n",
    "            #self.bias_output += delta_output\n",
    "        \n",
    "            losses.append(np.sum(np.square(error)))\n",
    "            #print(np.square(error))\n",
    "            print(\"predicted\",softmax_activation)\n",
    "            print(\"expected\",y_train[i])\n",
    "        \n",
    "        \n",
    "        #print(\"delta output\", delta_output)\n",
    "        #print(\"hidden layer error\", hidden_layer_error)\n",
    "        #print(\"hidden layer delta\", hidden_layer_delta)\n",
    "        #print(\"second weights\", self.weights_input)\n",
    "        print(\"weights\",self.weights_output)\n",
    "\n",
    "        print(\"softmax derivative\", self.softmax_derivative(softmax_activation))\n",
    "        #print(\"sigmoid activation\", sigmoid_activation)\n",
    "        #print(\"softmax activation\", softmax_activation)\n",
    "        #print(\"error\", error)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward_sigmoid(self, x):\n",
    "        return self.sigmoid(np.dot(self.weights_input, x))#+self.bias_input)\n",
    "    def sigmoid_test(self, x):\n",
    "        return self.sigmoid(np.dot(self.weights_output, x))#+self.bias_input)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x - x.max())\n",
    "        return e / e.sum()\n",
    "    def softmax_layer(self, data):\n",
    "        return self.softmax(np.dot(self.weights_output, data))#+ self.bias_output)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        # Taken from https://stackoverflow.com/questions/54976533/derivative-of-softmax-function-in-python\n",
    "        s = x.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "\n",
    "    def SGD(self, epochs, batch_size):\n",
    "        lmao = 1\n",
    "\n",
    "    def nupyzdintas(self):\n",
    "        for i in range(64):\n",
    "            sigmoid_activation = self.forward_sigmoid(x_train[i])\n",
    "            softmax_activation = self.sigmoid_test(sigmoid_activation)\n",
    "            # Mean squared error function\n",
    "            error = softmax_activation-y_train[i]\n",
    "        \n",
    "            delta_output =  np.dot(error, self.sigmoid_derivative(softmax_activation))\n",
    "            \n",
    "            #delta_output = np.outer(sigmoid_activation, np.dot(error,self.softmax_derivative(softmax_activation)))\n",
    "            self.bias_output += error\n",
    "            \n",
    "            delta_input = np.dot(delta_output, self.weights_output)\n",
    "            temp = np.dot(delta_input, self.sigmoid_derivative(sigmoid_activation))\n",
    "            \n",
    "            \n",
    "            self.weights_output += np.outer(sigmoid_activation, delta_output).T * self.learning_rate\n",
    "            print(\"delta output\", delta_output)\n",
    "            print(\"delta input\", delta_input)\n",
    "            print(\"tmp\", temp)\n",
    "            self.weights_input += np.outer(x_train[0], temp) * self.learning_rate\n",
    "            self.bias_input += delta_input\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "\n",
    "test = NeuralNetwork()\n",
    "test.__innit__()\n",
    "test.nupyzdintas()\n",
    "#test.training()\n",
    "#test.test()\n",
    "plt.plot(losses)\n",
    "plt.show\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sigmoid layer\n",
    "#epoch (means passing the entire data set. One epoch contains number_of_items/batch_size iterations)\n",
    "#forward pass (refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.)\n",
    "#backward pass (refers to process of counting changes in weights (de facto learning), using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.)\n",
    "\n",
    "\n",
    "\n",
    "#Neural Networks\n",
    "#Parameterized number and types of layers. number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidLayer(units, x):\n",
    "    weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.53619004],\n",
      "       [ 1.56651314],\n",
      "       [ 1.80161131]]), array([[0.56628081]])]\n",
      "[ 9.60957939e-01  2.03613961e-01 -8.01874742e-01 -5.65171013e-01\n",
      "  1.41022351e+00 -1.95997209e+00 -1.66705420e-02  1.12882664e+00\n",
      " -3.49296312e-01 -1.54713477e+00  5.17084054e-01 -1.45649878e+00\n",
      " -7.32995819e-01 -4.55096516e-01  8.91261789e-01  2.14952221e+00\n",
      " -1.59823079e+00 -9.64555671e-02 -1.24804751e+00 -9.84556013e-01\n",
      "  3.20040197e-02 -1.30015600e+00  2.42689258e-01  6.90503425e-01\n",
      "  1.13433117e+00 -2.73671486e-01  6.39863072e-01 -7.16442635e-01\n",
      " -1.33345848e-01  3.31146707e-01 -8.13844892e-01  5.59047108e-01\n",
      " -8.31024119e-01  8.80314984e-01 -3.48237999e-01 -2.56118282e-01\n",
      "  9.25833186e-01  2.75111702e-02  2.12381498e-01 -6.99500034e-02\n",
      "  1.04341481e-01  2.41754096e-01  4.78163883e-01  4.78288056e-02\n",
      " -1.76387881e-01  1.52010685e+00 -9.13495203e-01 -1.43378653e+00\n",
      "  1.12415405e+00 -8.33291985e-01 -9.43379121e-01  4.20352415e-01\n",
      "  8.43742956e-01 -6.12304078e-01 -3.06407495e-01  1.58556496e+00\n",
      "  7.71753178e-01  1.39750790e+00 -1.00958532e+00 -1.17704249e+00\n",
      " -5.14572763e-01  4.64331755e-01  2.08806047e+00  8.71018599e-01\n",
      " -1.28281037e+00 -2.65389572e+00 -1.23620598e+00  1.92148989e-01\n",
      " -8.69216974e-01  3.84146587e-01  2.06240378e-01  1.36912247e+00\n",
      "  3.92845562e-01 -3.55522194e-01  1.80526269e+00 -1.10410849e+00\n",
      "  6.56612706e-01  1.10951330e-01 -7.88095357e-01  1.48124907e+00\n",
      " -7.14265530e-01 -1.89409104e+00  6.99601722e-02 -1.45339597e+00\n",
      " -2.94790588e-03  1.87980089e-01  7.69452181e-01 -3.77478378e-01\n",
      " -4.46902069e-01 -5.34784674e-01 -2.55139698e-01 -3.64215743e-01\n",
      "  2.02304117e-01 -4.24866844e-01 -3.66704293e-01 -8.33422944e-01\n",
      "  8.04820282e-01 -2.74689754e-01 -8.87496806e-01 -1.63765811e+00\n",
      " -1.98673190e+00  4.74927627e-01  6.61388321e-01 -1.38966612e-01\n",
      "  1.61057468e+00 -9.92709074e-01  1.37883973e+00  2.05970376e-01\n",
      " -1.66395342e+00 -8.91422269e-01  3.24519256e+00  1.75895052e+00\n",
      " -8.19369562e-01 -4.89484953e-01 -2.14756013e+00 -2.16244425e-01\n",
      "  9.48692856e-02 -9.27934655e-01  5.30641825e-01  2.98882906e-01\n",
      "  5.01860068e-01 -2.02019128e+00 -4.94726209e-01 -1.81786675e+00\n",
      " -1.05551559e+00  2.13387352e-01 -4.91399248e-01  3.63711276e-01\n",
      " -1.55513282e-01 -1.44236330e+00 -1.83833577e+00 -4.01584592e-02\n",
      " -1.70341271e+00 -1.06171807e+00 -3.83123304e-01  3.60280343e-01\n",
      " -1.16641098e+00 -6.86278547e-01 -9.35928563e-01  1.02387280e+00\n",
      "  2.11414329e+00  1.02820535e-01  3.19544709e-01  7.45616620e-01\n",
      "  6.65678157e-01  1.74572582e-01 -1.03427861e+00  5.47088590e-01\n",
      "  4.13716884e-02  1.62300509e+00 -9.87346982e-01  6.30820925e-01\n",
      "  2.37108751e-01 -1.69842841e-01 -1.76735673e-01 -8.00868926e-01\n",
      " -7.78345165e-01  1.70678584e+00  8.90401044e-01  8.63609805e-01\n",
      " -8.65432262e-01 -9.79180138e-01  5.05091792e-01  1.12806197e+00\n",
      "  2.25656249e+00 -2.14809899e-01  3.98091822e-02 -9.79958057e-01\n",
      " -2.58721871e-01 -1.63515259e+00 -9.79629211e-01  2.25691728e+00\n",
      "  1.16446093e+00  1.98531827e-01  2.02832651e+00  9.82129492e-01\n",
      "  1.23444183e+00  1.29436157e+00  2.27129966e+00 -2.06278277e+00\n",
      " -3.27471575e-01  5.88834624e-01 -1.47449620e+00  1.18020999e+00\n",
      " -1.70683542e+00 -3.49459962e-01  2.49993570e-01  3.85293323e-01\n",
      "  1.66579133e+00 -1.00790851e+00 -8.82652534e-01  2.24568975e-02\n",
      " -7.87569029e-01  1.50923461e+00 -7.36333102e-02  7.21682715e-01\n",
      " -1.09730722e+00 -1.07563930e+00 -1.11069260e+00 -2.03042852e+00\n",
      "  7.67729235e-01  4.36211706e-01 -1.17785534e+00  3.05609497e-01\n",
      " -5.69546256e-01 -2.51650953e+00 -1.98492409e+00  2.14937679e-01\n",
      "  1.07586151e+00 -7.48211634e-01 -5.81153983e-01  3.45590627e-02\n",
      "  3.96989842e-01  1.71665159e+00 -3.29866541e-01 -1.23590018e+00\n",
      " -5.22041794e-01 -1.76113168e-01  8.27961446e-01 -1.30126554e+00\n",
      " -8.75054790e-01  1.97238078e-01  9.23067547e-01  2.04837297e+00\n",
      " -1.43731373e+00  3.80616968e-01 -8.02123681e-01 -1.20883549e+00\n",
      "  1.23200465e+00  9.94198998e-01 -3.08703789e-01 -3.68550486e-01\n",
      "  4.38067130e-01 -1.58836256e+00  3.00866484e-01 -5.57278107e-01\n",
      "  6.16353575e-03  4.44336987e-01 -1.42245655e+00 -8.43918680e-02\n",
      "  1.30628609e+00  2.91864653e-01  5.14941899e-02  1.70264449e+00\n",
      " -1.33364790e+00  1.07546469e+00 -3.39771978e+00 -7.89155359e-01\n",
      " -1.81506102e-01 -7.07818308e-01  1.95291356e+00 -1.17601573e+00\n",
      " -1.38044347e+00 -7.88696888e-01 -1.17405991e-01 -8.80980912e-01\n",
      " -1.88470756e+00 -1.38847833e-01  1.79056252e-01 -5.80067634e-01\n",
      " -1.27728241e+00  3.04290638e-01 -1.50413450e+00  2.30842859e-01\n",
      "  1.02743436e+00  6.36757007e-01 -1.23730737e+00  1.56788656e+00\n",
      "  1.33210971e+00 -4.81867273e-01  2.37592207e-01 -1.60957594e-01\n",
      " -7.20689993e-01 -1.89627146e-02 -1.30827037e+00 -1.83043800e+00\n",
      "  6.97606646e-01  3.49770209e-01 -2.79999587e-01 -1.10614715e+00\n",
      " -7.38768595e-02  1.04442022e+00  6.45085384e-01  1.40789095e+00\n",
      " -1.54160843e+00 -4.76508788e-01 -6.34343283e-01 -1.70339865e+00\n",
      " -3.82942701e-01 -1.37052088e-01  1.52094511e+00  1.66910669e-01\n",
      "  1.07490151e+00 -1.12070064e+00 -2.98267346e-01 -2.18944877e-01\n",
      " -7.65326171e-01  4.23502044e-01  6.78283461e-01  6.22561630e-01\n",
      " -2.71401892e+00  7.67703984e-01 -2.09728300e+00  2.64035806e-01\n",
      " -3.55087636e-01  1.32570685e+00  1.87801815e+00  2.31223756e-01\n",
      "  5.94442512e-02 -1.93579739e-01  2.14533722e+00  2.66122001e-01\n",
      "  4.92393878e-01  3.50156112e-01  1.43043620e+00  7.32248024e-02\n",
      " -3.45310301e-01  1.61646880e+00 -5.39550216e-01  2.26598405e-01\n",
      "  4.44594445e-01  9.04066888e-01 -1.27732627e+00  1.42760976e+00\n",
      "  5.63229422e-02 -2.13728656e-01 -1.90463600e+00  8.68903490e-01\n",
      "  3.91411863e-01 -2.48844655e-01  4.82263525e-01 -6.05537689e-01\n",
      "  1.32436006e+00  1.22015098e-01  8.49026138e-01  1.13135877e+00\n",
      "  4.64876015e-01  7.89169642e-01 -6.99988721e-01  2.40954201e+00\n",
      " -7.36081203e-01 -1.40204890e-01 -6.01276338e-01 -5.50806201e-01\n",
      " -8.11847749e-01 -6.68521276e-02 -1.69003871e+00  8.59208303e-01\n",
      "  9.89174268e-01  1.58608711e+00 -1.47477017e-01 -2.23543525e-01\n",
      "  5.62576847e-01  6.73995705e-01 -1.01004530e+00  2.46382049e-01\n",
      " -5.16470825e-01  1.01705190e+00 -4.10941094e-01 -9.47651159e-01\n",
      "  3.05919089e-01 -5.16712472e-01 -3.24533092e-01 -1.16497150e+00\n",
      "  8.89105613e-01 -3.27250747e-01  1.05807882e+00 -2.22034113e+00\n",
      " -3.71944090e-01 -1.83036894e-01 -3.09572520e-01  1.47738827e+00\n",
      "  1.26854593e+00  1.29903345e+00 -7.42434989e-01  4.15663714e-01\n",
      "  2.36109471e-01  2.16449499e-01 -1.77610047e-01 -3.98808516e-01\n",
      " -1.24718237e+00 -1.63724140e+00 -1.47638908e-01 -4.44450476e-01\n",
      "  2.67880638e-01 -1.22460173e+00 -4.87298044e-01 -4.26739735e-01\n",
      " -3.18119939e-01  1.80803236e+00  7.94875307e-01 -4.42927491e-01\n",
      " -1.61832866e+00  5.80645088e-02  1.74262433e-01 -2.71636133e-03\n",
      "  3.42802884e-01  1.86934367e-01  9.14364235e-01  1.14699842e+00]\n"
     ]
    }
   ],
   "source": [
    "sizes = [2, 3, 1]\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "print(biases)\n",
    "\n",
    "weights = np.random.normal(size=400)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration # 0\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.84365544]\n",
      " [0.89140871]\n",
      " [0.87749095]\n",
      " [0.90791675]]\n",
      "Loss: \n",
      "0.39071696695047575\n",
      "\n",
      "\n",
      "for iteration # 100\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.47487267]\n",
      " [0.49405257]\n",
      " [0.53785013]\n",
      " [0.52405324]]\n",
      "Loss: \n",
      "0.24242528626364135\n",
      "\n",
      "\n",
      "for iteration # 200\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.19598313]\n",
      " [0.60960974]\n",
      " [0.76024414]\n",
      " [0.41812489]]\n",
      "Loss: \n",
      "0.10578131045642392\n",
      "\n",
      "\n",
      "for iteration # 300\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.06807624]\n",
      " [0.86772868]\n",
      " [0.88472276]\n",
      " [0.15219759]]\n",
      "Loss: \n",
      "0.014645756322074838\n",
      "\n",
      "\n",
      "for iteration # 400\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.04069718]\n",
      " [0.91411273]\n",
      " [0.92250742]\n",
      " [0.10045333]]\n",
      "Loss: \n",
      "0.006282213448486016\n",
      "\n",
      "\n",
      "for iteration # 500\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.03012717]\n",
      " [0.93307285]\n",
      " [0.93874289]\n",
      " [0.07888079]]\n",
      "Loss: \n",
      "0.0038403755715479687\n",
      "\n",
      "\n",
      "for iteration # 600\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02443289]\n",
      " [0.94371636]\n",
      " [0.94804454]\n",
      " [0.06665583]]\n",
      "Loss: \n",
      "0.002726795920636829\n",
      "\n",
      "\n",
      "for iteration # 700\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02082333]\n",
      " [0.95067613]\n",
      " [0.95420149]\n",
      " [0.05861315]]\n",
      "Loss: \n",
      "0.0020998650661874377\n",
      "\n",
      "\n",
      "for iteration # 800\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01830573]\n",
      " [0.95565238]\n",
      " [0.95864131]\n",
      " [0.05283706]]\n",
      "Loss: \n",
      "0.00170102680150904\n",
      "\n",
      "\n",
      "for iteration # 900\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01643598]\n",
      " [0.95942504]\n",
      " [0.96202897]\n",
      " [0.04844288]]\n",
      "Loss: \n",
      "0.001426245021447225\n",
      "\n",
      "\n",
      "for iteration # 1000\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01498443]\n",
      " [0.96240587]\n",
      " [0.96471932]\n",
      " [0.04496121]]\n",
      "Loss: \n",
      "0.001226022167507708\n",
      "\n",
      "\n",
      "for iteration # 1100\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01381979]\n",
      " [0.96483446]\n",
      " [0.96692048]\n",
      " [0.04211789]]\n",
      "Loss: \n",
      "0.0010739431957707127\n",
      "\n",
      "\n",
      "for iteration # 1200\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01286129]\n",
      " [0.96686054]\n",
      " [0.96876337]\n",
      " [0.03974105]]\n",
      "Loss: \n",
      "0.0009546786104876755\n",
      "\n",
      "\n",
      "for iteration # 1300\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01205632]\n",
      " [0.96858293]\n",
      " [0.97033481]\n",
      " [0.03771693]]\n",
      "Loss: \n",
      "0.0008587443185261937\n",
      "\n",
      "\n",
      "for iteration # 1400\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01136909]\n",
      " [0.97006972]\n",
      " [0.97169496]\n",
      " [0.03596696]]\n",
      "Loss: \n",
      "0.0007799688168850791\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each row is a training example, each column is a feature  [X1, X2, X3]\n",
    "X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "# Define useful functions    \n",
    "\n",
    "# Activation function\n",
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(p):\n",
    "    return p * (1 - p)\n",
    "\n",
    "# Class definition\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x,y):\n",
    "        self.input = x\n",
    "        self.weights1= np.random.rand(self.input.shape[1],4) # considering we have 4 nodes in the hidden layer\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y = y\n",
    "        self.output = np. zeros(y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        return self.layer2\n",
    "        \n",
    "    def backprop(self):\n",
    "        d_weights2 = np.dot(self.layer1.T, 2*(self.y -self.output)*sigmoid_derivative(self.output))\n",
    "        d_weights1 = np.dot(self.input.T, np.dot(2*(self.y -self.output)*sigmoid_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))\n",
    "    \n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.output = self.feedforward()\n",
    "        self.backprop()\n",
    "        \n",
    "\n",
    "NN = NeuralNetwork(X,y)\n",
    "for i in range(1500): # trains the NN 1,000 times\n",
    "    if i % 100 ==0: \n",
    "        print (\"for iteration # \" + str(i) + \"\\n\")\n",
    "        print (\"Input : \\n\" + str(X))\n",
    "        print (\"Actual Output: \\n\" + str(y))\n",
    "        print (\"Predicted Output: \\n\" + str(NN.feedforward()))\n",
    "        print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.feedforward())))) # mean sum squared loss\n",
    "        print (\"\\n\")\n",
    "  \n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
